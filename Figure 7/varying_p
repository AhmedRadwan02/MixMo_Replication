import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import argparse
import numpy as np
from tqdm import tqdm
import json
from torch.utils.data import Subset, DataLoader
from data_handler import DataHandler
from models import Wide_ResNet28

def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True

def get_model(args, num_classes):
    aug_type = 'LinearMixMo' if args.approach == 'linear_mixmo_cutmix' else 'CutMixMo'
    return Wide_ResNet28(
        widen_factor=args.width,
        dropout_rate=0.3,
        num_classes=num_classes,
        augmentation_type=aug_type,
    )

def mixmo_loss(outputs, targets, kappa=None, r=3.0):
    out1, out2, out_mix1, out_mix2 = outputs
    y1, y2 = targets
    criterion = nn.CrossEntropyLoss(reduction='none')
    loss1 = criterion(out_mix1, y1)
    loss2 = criterion(out_mix2, y2)

    if kappa is not None:
        weight1 = 2 * (kappa ** (1 / r)) / ((kappa ** (1 / r)) + ((1 - kappa) ** (1 / r)))
        weight2 = 2 * ((1 - kappa) ** (1 / r)) / ((kappa ** (1 / r)) + ((1 - kappa) ** (1 / r)))
        loss1 = (loss1 * weight1).mean()
        loss2 = (loss2 * weight2).mean()
    else:
        loss1 = loss1.mean()
        loss2 = loss2.mean()

    return loss1 + loss2

def compute_dre(pred1, pred2, true_targets):
    err1 = pred1.ne(true_targets)
    err2 = pred2.ne(true_targets)
    different_errors = (err1 & err2 & pred1.ne(pred2)).sum().item()
    simultaneous_errors = (err1 & err2).sum().item()
    if simultaneous_errors == 0:
        return 0.0
    return different_errors / simultaneous_errors

def evaluate(model, test_loader, device):
    model.eval()
    correct_ensemble = 0
    correct1 = 0
    correct2 = 0
    total = 0
    dre_sum = 0.0
    batch_count = 0

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            # In evaluation, the same input is repeated to obtain ensemble predictions.
            out1, out2, _, _, _ = model(inputs, inputs)
            ensemble_output = (out1 + out2) / 2

            _, predicted_ensemble = ensemble_output.max(1)
            _, predicted1 = out1.max(1)
            _, predicted2 = out2.max(1)

            correct_ensemble += predicted_ensemble.eq(targets).sum().item()
            correct1 += predicted1.eq(targets).sum().item()
            correct2 += predicted2.eq(targets).sum().item()

            dre_sum += compute_dre(predicted1, predicted2, targets)
            batch_count += 1

            total += targets.size(0)

    acc_ensemble = 100. * correct_ensemble / total
    acc1 = 100. * correct1 / total
    acc2 = 100. * correct2 / total
    diversity = dre_sum / batch_count if batch_count > 0 else 0.0
    return acc_ensemble, (acc1 + acc2) / 2, diversity

def train_and_save_model(p, args, train_loader, model_path, device, num_classes):
    model = get_model(args, num_classes).to(device)
    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[150, 225], gamma=0.1)
    history = []

    model.train()
    for epoch in range(300):
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            batch_size = inputs.size(0) // 2
            x1, x2 = torch.chunk(inputs, 2)
            y1, y2 = torch.chunk(targets, 2)
            x1, x2, y1, y2 = x1.to(device), x2.to(device), y1.to(device), y2.to(device)

            if hasattr(model, 'set_mixing_prob'):
                model.set_mixing_prob(p)
            if hasattr(model, 'set_mixing_mode'):
                # Sample mixing mode based on probability p: use 'patch' with probability p, else 'linear'
                use_patch_mixing = (torch.rand(1).item() < p)
                model.set_mixing_mode('patch' if use_patch_mixing else 'linear')

            out1, out2, out_mix1, out_mix2, kappa = model(x1, x2)
            loss = mixmo_loss((out1, out2, out_mix1, out_mix2), (y1, y2), kappa)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        scheduler.step()

        if epoch >= 290:
            acc_ensemble, acc_individual, diversity = evaluate(model, train_loader, device)
            history.append((acc_ensemble, acc_individual, diversity))

    torch.save(model.state_dict(), model_path)
    avg_metrics = np.mean(np.array(history), axis=0)
    return avg_metrics[0], avg_metrics[1], avg_metrics[2]

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    set_seed(42)
    data_handler = DataHandler(data_root="/path/to/data")
    train_loader, test_loader = data_handler.get_cifar100(batch_size=128, batch_repetitions=2)
    num_classes = 100
    results = []

    for p in np.arange(0, 1.1, 0.1):
        print(f"\nProcessing patch mixing probability p = {p:.1f}")
        # Create a dummy args object with the needed attributes.
        class Args: pass
        args = Args()
        args.approach = 'cut_mixmo_cutmix'
        args.width = 10

        model_path = f"./saved_models/model_p_{p:.1f}.pth"
        os.makedirs("./saved_models", exist_ok=True)

        if os.path.exists(model_path):
            print("Model exists. Loading and evaluating...")
            model = get_model(args, num_classes).to(device)
            model.load_state_dict(torch.load(model_path))
            if hasattr(model, 'set_mixing_prob'):
                model.set_mixing_prob(p)
            if hasattr(model, 'set_mixing_mode'):
                use_patch_mixing = (torch.rand(1).item() < p)
                model.set_mixing_mode('patch' if use_patch_mixing else 'linear')
            acc_ensemble, acc_individual, diversity = evaluate(model, test_loader, device)
        else:
            print("Model not found. Training...")
            acc_ensemble, acc_individual, diversity = train_and_save_model(p, args, train_loader, model_path, device, num_classes)

        results.append({
            'p': round(p, 1),
            'ensemble_acc': acc_ensemble,
            'individual_acc': acc_individual,
            'diversity_dre': diversity
        })

    with open("mixmo_diversity_accuracy_results.json", "w") as f:
        json.dump(results, f, indent=4)

if __name__ == "__main__":
    main()
